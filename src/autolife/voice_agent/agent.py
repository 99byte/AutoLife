"""
è¯­éŸ³ä»£ç† - æ•´åˆ AutoGLM å’Œè¯­éŸ³äº¤äº’èƒ½åŠ›
"""

import os
import sys
from pathlib import Path
from typing import Callable

# æ·»åŠ  Open-AutoGLM åˆ° Python è·¯å¾„
AUTOGLM_PATH = Path(__file__).parent.parent.parent.parent / "Open-AutoGLM"
sys.path.insert(0, str(AUTOGLM_PATH))

from phone_agent import PhoneAgent
from phone_agent.agent import AgentConfig
from phone_agent.model import ModelConfig

from autolife.voice_agent.asr import ASRBase, ZhipuASR
from autolife.voice_agent.tts import TTSBase, ZhipuTTS


class VoiceAgent:
    """
    è¯­éŸ³æ™ºèƒ½åŠ©æ‰‹

    æ•´åˆ AutoGLM çš„æ‰‹æœºæ§åˆ¶èƒ½åŠ›å’Œè¯­éŸ³äº¤äº’èƒ½åŠ›,
    é€šè¿‡è¯­éŸ³æŒ‡ä»¤æ§åˆ¶æ‰‹æœºå®Œæˆå„ç§ä»»åŠ¡ã€‚

    ç‰¹æ€§:
    - è¯­éŸ³è¾“å…¥ (ASR)
    - è¯­éŸ³è¾“å‡º (TTS)
    - å•æ¬¡äº¤äº’æ¨¡å¼ï¼ˆæŒ‰é’®è§¦å‘ï¼‰
    - æŒç»­å¯¹è¯æ¨¡å¼ï¼ˆè‡ªåŠ¨åˆ†æ®µï¼‰
    - VAD è¯­éŸ³æ´»åŠ¨æ£€æµ‹
    - å¤šæ¨¡æ€ç†è§£ (è¯­éŸ³ + å±å¹•è§†è§‰)
    - è¿ç»­å¯¹è¯èƒ½åŠ›

    ç¤ºä¾‹:
        >>> from autolife import VoiceAgent
        >>>
        >>> # åˆ›å»ºè¯­éŸ³åŠ©æ‰‹
        >>> agent = VoiceAgent()
        >>>
        >>> # å•æ¬¡äº¤äº’æ¨¡å¼
        >>> agent.run_single_interaction()
        >>>
        >>> # æŒç»­å¯¹è¯æ¨¡å¼
        >>> agent.run_continuous_interaction()
        >>>
        >>> # æˆ–ä»æ–‡æœ¬è¾“å…¥
        >>> agent.run_from_text("å¸®æˆ‘æœç´¢é™„è¿‘çš„é¤å…")
    """

    def __init__(
        self,
        # AutoGLM é…ç½®
        model_config: ModelConfig | None = None,
        agent_config: AgentConfig | None = None,
        # è¯­éŸ³æ¨¡å—é…ç½®
        asr_client: ASRBase | None = None,
        tts_client: TTSBase | None = None,
        # å›è°ƒå‡½æ•°
        confirmation_callback: Callable[[str], bool] | None = None,
        takeover_callback: Callable[[str], None] | None = None,
        # è¯­éŸ³åé¦ˆå¼€å…³
        enable_voice_feedback: bool = True,
    ):
        """
        åˆå§‹åŒ–è¯­éŸ³åŠ©æ‰‹

        Args:
            model_config: AutoGLM æ¨¡å‹é…ç½®
            agent_config: AutoGLM ä»£ç†é…ç½®
            asr_client: ASR å®¢æˆ·ç«¯,é»˜è®¤ä½¿ç”¨ ZhipuASR
            tts_client: TTS å®¢æˆ·ç«¯,é»˜è®¤ä½¿ç”¨ ZhipuTTS
            confirmation_callback: æ•æ„Ÿæ“ä½œç¡®è®¤å›è°ƒ
            takeover_callback: äººå·¥æ¥ç®¡å›è°ƒ
            enable_voice_feedback: æ˜¯å¦å¯ç”¨è¯­éŸ³åé¦ˆ
        """
        # å¦‚æœæ²¡æœ‰æä¾› model_configï¼Œä»ç¯å¢ƒå˜é‡åˆ›å»º
        if model_config is None:
            model_config = ModelConfig(
                base_url=os.getenv("AUTOGLM_BASE_URL", "http://localhost:8000/v1"),
                api_key=os.getenv("AUTOGLM_API_KEY", "EMPTY"),
                model_name=os.getenv("AUTOGLM_MODEL", "autoglm-phone-9b"),
            )

        # åˆå§‹åŒ– AutoGLM
        self.phone_agent = PhoneAgent(
            model_config=model_config,
            agent_config=agent_config,
            confirmation_callback=confirmation_callback,
            takeover_callback=takeover_callback,
        )

        # åˆå§‹åŒ–è¯­éŸ³æ¨¡å—
        self.asr = asr_client or ZhipuASR()
        self.tts = tts_client or ZhipuTTS()

        self.enable_voice_feedback = enable_voice_feedback

        # ä¼šè¯çŠ¶æ€
        self.is_active = False
        self.conversation_history = []

    def run_from_text(self, task: str, speak_result: bool = True) -> str:
        """
        ä»æ–‡æœ¬æŒ‡ä»¤æ‰§è¡Œä»»åŠ¡

        Args:
            task: ä»»åŠ¡æè¿°
            speak_result: æ˜¯å¦æœ—è¯»ç»“æœ

        Returns:
            str: æ‰§è¡Œç»“æœæ¶ˆæ¯
        """
        print(f"\n[ç”¨æˆ·] {task}")

        # æ‰§è¡Œä»»åŠ¡
        result = self.phone_agent.run(task)

        # è¯­éŸ³åé¦ˆ
        if speak_result and self.enable_voice_feedback:
            self.tts.speak(result)
        else:
            print(f"[åŠ©æ‰‹] {result}")

        # è®°å½•å†å²
        self.conversation_history.append({"role": "user", "content": task})
        self.conversation_history.append({"role": "assistant", "content": result})

        return result

    def run_from_voice(self, audio_input) -> str:
        """
        ä»è¯­éŸ³æŒ‡ä»¤æ‰§è¡Œä»»åŠ¡

        Args:
            audio_input: éŸ³é¢‘è¾“å…¥ (æ–‡ä»¶è·¯å¾„æˆ–éŸ³é¢‘æ•°æ®)

        Returns:
            str: æ‰§è¡Œç»“æœæ¶ˆæ¯
        """
        # è¯­éŸ³è¯†åˆ«
        asr_result = self.asr.transcribe(audio_input)
        task = asr_result.text

        print(f"\n[è¯­éŸ³è¯†åˆ«] {task} (ç½®ä¿¡åº¦: {asr_result.confidence:.2f})")

        # æ‰§è¡Œä»»åŠ¡
        return self.run_from_text(task, speak_result=True)

    def run_single_interaction(self, duration: float = 5.0) -> str:
        """
        å•æ¬¡è¯­éŸ³äº¤äº’

        å‰ç«¯ç‚¹å‡»æŒ‰é’® â†’ å½•éŸ³ duration ç§’ â†’ ASR è¯†åˆ« â†’ æ‰§è¡Œä»»åŠ¡ â†’ TTS åé¦ˆ

        Args:
            duration: å½•éŸ³æ—¶é•¿ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤ 5 ç§’

        Returns:
            str: æ‰§è¡Œç»“æœ
        """
        from autolife.voice_agent.audio import AudioRecorder
        import tempfile

        recorder = AudioRecorder()

        print(f"[å½•éŸ³ä¸­] è¯·è¯´å‡ºä½ çš„æŒ‡ä»¤ï¼ˆ{duration} ç§’ï¼‰...")
        audio = recorder.record_for_duration(duration)

        # ä¿å­˜ä¸´æ—¶æ–‡ä»¶
        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as f:
            temp_path = Path(f.name)
            recorder.save_to_file(audio, temp_path)

        try:
            # ASR è¯†åˆ«å¹¶æ‰§è¡Œ
            result = self.run_from_voice(temp_path)
            return result
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            temp_path.unlink(missing_ok=True)

    def run_continuous_interaction(self) -> None:
        """
        æŒç»­å¯¹è¯æ¨¡å¼

        å‰ç«¯ç‚¹å‡»å¼€å§‹ â†’ æŒç»­ç›‘å¬ â†’ VAD æ£€æµ‹è¯­éŸ³åœé¡¿ â†’ è‡ªåŠ¨åˆ†æ®µè¯†åˆ« â†’ æ‰§è¡Œ
        â†’ ç»§ç»­ç›‘å¬ â†’ ç‚¹å‡»åœæ­¢é€€å‡º

        ä½¿ç”¨ VADï¼ˆè¯­éŸ³æ´»åŠ¨æ£€æµ‹ï¼‰è‡ªåŠ¨åˆ†æ®µï¼Œæ— éœ€å”¤é†’è¯
        """
        from autolife.voice_agent.audio import AudioRecorder
        import tempfile

        self.is_active = True
        recorder = AudioRecorder()

        print("\nğŸ¤ æŒç»­å¯¹è¯æ¨¡å¼å·²å¯åŠ¨")
        print("   å¼€å§‹è¯´è¯ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨æ£€æµ‹è¯­éŸ³åœé¡¿")
        print("   æŒ‰ Ctrl+C é€€å‡º\n")

        try:
            while self.is_active:
                # ä½¿ç”¨ VAD æ£€æµ‹è¯­éŸ³æ´»åŠ¨
                audio = self._record_until_silence(recorder)

                if len(audio) > 0:
                    # ä¿å­˜å¹¶è¯†åˆ«
                    with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as f:
                        temp_path = Path(f.name)
                        recorder.save_to_file(audio, temp_path)

                    try:
                        self.run_from_voice(temp_path)
                    except Exception as e:
                        print(f"[é”™è¯¯] {e}")
                    finally:
                        temp_path.unlink(missing_ok=True)

        except KeyboardInterrupt:
            print("\n\næ­£åœ¨é€€å‡º...")
            self.stop_listening()

    def _record_until_silence(
        self,
        recorder,
        chunk_duration: float = 0.5,
        silence_threshold: float = 0.01,
        silence_duration: float = 2.0,
    ):
        """
        å½•éŸ³ç›´åˆ°æ£€æµ‹åˆ°é™éŸ³

        Args:
            recorder: å½•éŸ³å™¨
            chunk_duration: æ¯æ¬¡å½•éŸ³å—çš„æ—¶é•¿ï¼ˆç§’ï¼‰
            silence_threshold: é™éŸ³é˜ˆå€¼ï¼ˆéŸ³é‡ï¼‰
            silence_duration: é™éŸ³æŒç»­æ—¶é•¿ï¼ˆç§’ï¼‰æ‰åœæ­¢

        Returns:
            np.ndarray: å½•åˆ¶çš„éŸ³é¢‘æ•°æ®
        """
        import numpy as np

        audio_chunks = []
        silence_chunks = 0
        max_silence_chunks = int(silence_duration / chunk_duration)

        print("[ç›‘å¬ä¸­] ç­‰å¾…è¯­éŸ³è¾“å…¥...")

        while True:
            # å½•åˆ¶ä¸€å°æ®µ
            chunk = recorder.record_for_duration(chunk_duration)

            # è®¡ç®—éŸ³é‡
            volume = np.abs(chunk).mean()

            if volume > silence_threshold:
                # æœ‰å£°éŸ³
                audio_chunks.append(chunk)
                silence_chunks = 0
                print(".", end="", flush=True)  # æ˜¾ç¤ºå½•éŸ³ä¸­
            else:
                # é™éŸ³
                if len(audio_chunks) > 0:
                    # å·²ç»å½•åˆ°å£°éŸ³äº†ï¼Œå¼€å§‹è®¡æ•°é™éŸ³
                    silence_chunks += 1
                    audio_chunks.append(chunk)

                    if silence_chunks >= max_silence_chunks:
                        print("\n[æ£€æµ‹åˆ°] è¯­éŸ³ç»“æŸ")
                        break

        if len(audio_chunks) == 0:
            return np.array([])

        return np.concatenate(audio_chunks, axis=0)

    def start_listening(self) -> None:
        """
        å¯åŠ¨è¯­éŸ³ç›‘å¬æ¨¡å¼ï¼ˆå‘åå…¼å®¹ï¼Œä½¿ç”¨æŒç»­å¯¹è¯æ¨¡å¼ï¼‰

        å·²åºŸå¼ƒï¼šè¯·ä½¿ç”¨ run_continuous_interaction() ä»£æ›¿
        """
        print("[æç¤º] start_listening() å·²åºŸå¼ƒï¼Œä½¿ç”¨æŒç»­å¯¹è¯æ¨¡å¼")
        self.run_continuous_interaction()

    def stop_listening(self) -> None:
        """åœæ­¢è¯­éŸ³ç›‘å¬"""
        self.is_active = False
        print("\n[è¯­éŸ³åŠ©æ‰‹] å·²åœæ­¢")

    def clear_history(self) -> None:
        """æ¸…ç©ºå¯¹è¯å†å²"""
        self.conversation_history = []
        print("[è¯­éŸ³åŠ©æ‰‹] å¯¹è¯å†å²å·²æ¸…ç©º")

    def get_conversation_summary(self) -> str:
        """
        è·å–å¯¹è¯æ‘˜è¦

        Returns:
            str: å¯¹è¯æ‘˜è¦
        """
        if not self.conversation_history:
            return "æš‚æ— å¯¹è¯è®°å½•"

        summary = []
        for i, msg in enumerate(self.conversation_history, 1):
            role = "ç”¨æˆ·" if msg["role"] == "user" else "åŠ©æ‰‹"
            summary.append(f"{i}. [{role}] {msg['content'][:50]}...")

        return "\n".join(summary)
